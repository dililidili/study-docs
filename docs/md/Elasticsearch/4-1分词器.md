# 4.1、分词器

## 1.文档规范化

```
GET _analyze
{
  "analyzer": "english",
  "text": "This is a phone。NFC"
}
```

**输出**

```
{
  "tokens" : [
    {
      "token" : "phone",
      "start_offset" : 10,
      "end_offset" : 15,
      "type" : "<ALPHANUM>",
      "position" : 3
    },
    {
      "token" : "nfc",
      "start_offset" : 16,
      "end_offset" : 19,
      "type" : "<ALPHANUM>",
      "position" : 4
    }
  ]
}
```



## 2.中文字符过滤器(character filter)

### 设置字符串过滤器html_strip

```
#设置字符串过滤器html_strip:过滤html想关标签内容,escaped_tags:要保留的标签
PUT test1
{
  "settings": {
    "analysis": {
      "char_filter": {
        "test_char_filter":{
          "type":"html_strip",
          "escaped_tags":["div"]
        }
      },
      "analyzer": {
        "test_analyzer":{
          "tokenizer":"keyword",
          "char_filter":["test_char_filter"]
        }
      }
    }
  }
}

#使用定义的过滤器
GET /test1/_analyze
{
  "analyzer": "test_analyzer",
  "text": ["<html><body><div> this is <br/> a &nbsp; phone </div></body></html>"]
}
```

**输出：**

```
{
  "tokens" : [
    {
      "token" : """

<div> this is 
 a   phone </div>

""",
      "start_offset" : 0,
      "end_offset" : 67,
      "type" : "word",
      "position" : 0
    }
  ]
}
```

### 自定义过滤器mapping

```
#自定义过滤器mapping

PUT test2
{
  "settings": {
    "analysis": {
      "char_filter": {
        "test_char_filter":{
          "type":"mapping",
          "mappings":[
            "滚 => *","垃圾 => **"
            ]
        }
      },
      "analyzer": {
        "test_analyzer":{
          "tokenizer":"keyword",
          "char_filter":["test_char_filter"]
        }
      }
    }
  }
}
#使用定义的分词器
GET /test2/_analyze
{
  "analyzer": "test_analyzer",
  "text": ["你个垃圾给我滚吧"]
}
```

**输出**

```
{
  "tokens" : [
    {
      "token" : "你个**给我*吧",
      "start_offset" : 0,
      "end_offset" : 8,
      "type" : "word",
      "position" : 0
    }
  ]
}
```

### 正则替换过滤器 pattern_replace

```
PUT test3
{
  "settings": {
    "analysis": {
      "char_filter": {
        "test_char_filter":{
          "type":"pattern_replace",
          "pattern":"(\\d{3})\\d{4}(\\d{4})",
          "replacement":"$1****$2"
        }
      },
      "analyzer": {
        "test_analyzer":{
          "tokenizer":"keyword",
          "char_filter":["test_char_filter"]
        }
      }
    }
  }
}

#使用定义的分词器
GET /test3/_analyze
{
  "analyzer": "test_analyzer",
  "text": ["我的手机号是18522380337,别忘了"]
}
```

**输出**

```
{
  "tokens" : [
    {
      "token" : "我的手机号是185****0337,别忘了",
      "start_offset" : 0,
      "end_offset" : 21,
      "type" : "word",
      "position" : 0
    }
  ]
}
```

## 2.令牌过滤器(token filter)

**官方文档各种token filter类型:**https://www.elastic.co/guide/en/elasticsearch/reference/7.10/analysis-unique-tokenfilter.html

### 去重过滤器unique

```
PUT test4
{
  "settings": {
    "analysis": {
      "filter": {
        "test_token_filter":{
          "type":"unique"
        }
      },
      "analyzer": {
        "test_analyzer":{
          "tokenizer":"standard",
          "filter":["test_token_filter"]
        }
      }
    }
  }
}

GET /test4/_analyze
{
  "analyzer": "test_analyzer",
  "text": ["this is a phone phone phone"]
}
```

**输出**

```
{
  "tokens" : [
    {
      "token" : "this",
      "start_offset" : 0,
      "end_offset" : 4,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "is",
      "start_offset" : 5,
      "end_offset" : 7,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "a",
      "start_offset" : 8,
      "end_offset" : 9,
      "type" : "<ALPHANUM>",
      "position" : 2
    },
    {
      "token" : "phone",
      "start_offset" : 10,
      "end_offset" : 15,
      "type" : "<ALPHANUM>",
      "position" : 3
    }
  ]
}
```

## 3.分词器

### 3.1、自定义分词器

```
#自定义分词器
DELETE test_analyze
PUT test_analyze
{
  "settings": {
    "analysis": {
      "char_filter": {
        "test_char_filter": {
          "type": "mapping",
          "mappings": [
            "速腾 => 大众",
            "宝来 => 大众",
            "卡罗拉 => 丰田"
          ]
        }
      },
      "filter": {
        "test_filter": {
          "type": "stop",
          "stopwords": [
            "宝马",
            "奔驰"
          ]
        }
      },
      "tokenizer": {
        "test_tokenizer": {
          "type": "pattern",
          "pattern": "[、,.?!]"
        }
      },
      "analyzer": {
        "test_analyzer": {
          "type": "custom",
          "char_filter": [
            "test_char_filter"
          ],
          "filter":["test_filter"],
          "tokenizer": "test_tokenizer"
        }
      }
    }
  }
}

GET /test_analyze/_analyze
{
  "analyzer": "test_analyzer",
  "text": ["宝马、卡罗拉、速腾,都是普通车"]
}
```

**输出**

```
{
  "tokens" : [
    {
      "token" : "丰田",
      "start_offset" : 3,
      "end_offset" : 6,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "大众",
      "start_offset" : 7,
      "end_offset" : 9,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "都是豪车",
      "start_offset" : 10,
      "end_offset" : 14,
      "type" : "word",
      "position" : 3
    }
  ]
}
```

### 3.2、中文分词器(character filter)

#### ik下载安装

**ik分词器下载地址：[Releases · medcl/elasticsearch-analysis-ik · GitHub](https://github.com/medcl/elasticsearch-analysis-ik/releases)**

```
cd es/cd elasticsearch-7.10.0/
cd plugins/
mkdir ik
#传输文件然后解压
unzip elasticsearch-analysis-ik-7.10.0.zip
#修改plugin-descriptor.properties配置文件 elasticsearch.version=你的ES版本号(如果下载和es对应版本则不用修改)
vim plugin-descriptor.properties
#保存
#重启elasticsearch
```

#### IK文件描述

- IKAnalyzer.cfg.xml：IK分词配置文件
- 主词库：main.dic
- 英文停用词：stopword.dic，不会建立在倒排索引中
- 特殊词库：
  - quantifier.dic：计量单位等
  - suffix.dic：后缀名
  - surname.dic：百家姓
  - preposition：语气词

#### 使用

**Analyzer有两种:**

- ik_smart
- ik_max_word

```
GET _analyze
{
  "analyzer": "ik_max_word",
  "text":["我买了一辆大众车"]
}
```

**输出**

```
{
  "tokens" : [
    {
      "token" : "我",
      "start_offset" : 0,
      "end_offset" : 1,
      "type" : "CN_CHAR",
      "position" : 0
    },
    {
      "token" : "买了",
      "start_offset" : 1,
      "end_offset" : 3,
      "type" : "CN_WORD",
      "position" : 1
    },
    {
      "token" : "一辆",
      "start_offset" : 3,
      "end_offset" : 5,
      "type" : "CN_WORD",
      "position" : 2
    },
    {
      "token" : "一",
      "start_offset" : 3,
      "end_offset" : 4,
      "type" : "TYPE_CNUM",
      "position" : 3
    },
    {
      "token" : "辆",
      "start_offset" : 4,
      "end_offset" : 5,
      "type" : "COUNT",
      "position" : 4
    },
    {
      "token" : "大众",
      "start_offset" : 5,
      "end_offset" : 7,
      "type" : "CN_WORD",
      "position" : 5
    },
    {
      "token" : "车",
      "start_offset" : 7,
      "end_offset" : 8,
      "type" : "CN_CHAR",
      "position" : 6
    }
  ]
}
```

### 3.3、热更新

> config/IKAnalyzer.cfg.xml

```
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE properties SYSTEM "http://java.sun.com/dtd/properties.dtd">
<properties>
	<comment>IK Analyzer 扩展配置</comment>
	<!--用户可以在这里配置自己的扩展字典 -->
	<entry key="ext_dict">custom/mydict.dic;custom/single_word_low_freq.dic</entry>
	 <!--用户可以在这里配置自己的扩展停止词字典-->
	<entry key="ext_stopwords">custom/ext_stopword.dic</entry>
 	<!--用户可以在这里配置远程扩展字典 -->
	<entry key="remote_ext_dict">location</entry>
 	<!--用户可以在这里配置远程扩展停止词字典-->
	<entry key="remote_ext_stopwords">http://xxx.com/xxx.dic</entry>
</properties>
```

其中 `location` 是指一个 url，比如 `http://yoursite.com/getCustomDict`，该请求只需满足以下两点即可完成分词热更新。

1. 该 http 请求需要返回两个头部(header)，一个是 `Last-Modified`，一个是 `ETag`，这两者都是字符串类型，只要有一个发生变化，该插件就会去抓取新的分词进而更新词库。
2. 该 http 请求返回的内容格式是一行一个分词，换行符用 `\n` 即可。

满足上面两点要求就可以实现热更新分词了，不需要重启 ES 实例。

#### java代码示例：

##### 读取静态文件

```
@RestController
@RequestMapping (value = "/api")
public class HomeController {
  @RequestMapping (value ="hotWord")
  public void msbHotword (HttpServletResponse response, String address) throws I0Exception {
  	//address:c://test.text
    File file = new File (address);
    FileInputStream fis = new FileInputStream (file);
    byte[] buffer = new byte[(int) file.Length()];
    response.setContentType("text/plain;charset=utf-8");
    response.setHeader( s: "Last-Modified", String.value0f(buffer.length));
    response.setHeader( s: "ETag", String.value0f(buffer.length));
    int offset = 0;
    while (fis.read(buffer, offset, len: buffer.length - offset) != -1) {
    }
    OutputStream out = response.getOutputStream();
    out.write(buffer);
    out. flush();
    fis.close();
  }
}  
```

#### 使用ik直接读取mysql数据库

下载source code(zip)

解压后用开发工具(idea)打开后 修改关键类`Dictionary.class`

仿照`loadRemoteExtDict`方法编写读取mysql数据库